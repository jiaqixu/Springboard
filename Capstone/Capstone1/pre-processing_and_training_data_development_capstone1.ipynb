{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This jupyter notebook is used to pre-processing training data development for capstone 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are just repeated from steps of data wrangling. If you have any questions/concerns, please review here: <br>\n",
    "https://github.com/jiaqixu/Springboard/blob/master/Capstone/Capstone1/data_wrangling_capstone1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of \"sp500_matrix_utd.csv\" is: (711, 145)\n",
      "remove stocks: ['TIE', 'BMC', 'PCL', 'POM', 'GAS', 'EMC', 'HOT', 'HAR', 'CHK', 'SCG']\n",
      "The size of \"sp500_all_download_data_01-01-2005_12-31-2019_Adj Close.csv\" is: (3774, 626)\n",
      "remove stocks: ['TIE', 'BMC', 'PCL', 'POM', 'GAS', 'EMC', 'HOT', 'HAR', 'CHK', 'SCG']\n",
      "The size of \"sp500_all_download_data_01-01-2005_12-31-2019_Adj Open.csv\" is: (3774, 626)\n",
      "The size of \"sp500_all_download_data_01-01-2005_12-31-2019_Volume.csv\" is: (3774, 636)\n",
      "The size of \"sp500_index_data_01-01-2000_05-20-2020.csv\" is: (5128, 5)\n",
      "The size of return is:(3773, 627)\n",
      "There are total 12 training and trading sets.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from datetime import datetime,date\n",
    "from calendar import monthrange\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import style\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#os.getcwd()\n",
    "#deprecate runtime warning\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "prefix_market = 'sp500'\n",
    "#prefix_market = 'FTSE100'\n",
    "base_os = 'G:\\Data\\Machine_Learning_Library\\spring_board_data_science\\Capstone\\Capstone1\\data'\n",
    "#os1 = os.path.join(base_os, prefix_market,'cleaned_data')\n",
    "#os2 = os.path.join(base_os, prefix_market,'download_data_revise')\n",
    "start_date = '2005-01-01'\n",
    "end_date = '2020-01-01'\n",
    "##load all data\n",
    "filename = prefix_market + \"_matrix_utd.csv\"\n",
    "origdat_matrix = pd.read_csv(os.path.join(base_os,filename),sep=\",\",index_col='symbol')\n",
    "origdat_matrix.columns = origdat_matrix.columns.astype('datetime64[ns]')\n",
    "print(\"The size of \\\"{}\\\" is: {}\".format(filename, origdat_matrix.shape))\n",
    "\n",
    "## define function to remove extreme price, for price of stocks above 5000 (this is data download problem)\n",
    "def rm_extreme_price(dat, threshold_price = 5000):\n",
    "    ##clean origdat\n",
    "    tmp = dat.apply(lambda x:sum(x>threshold_price),axis=0)\n",
    "    rm_list = []\n",
    "    for i in range(tmp.shape[0]):\n",
    "        if tmp[i]>0:\n",
    "            rm_list.append(tmp.index[i])\n",
    "    print(\"remove stocks:\",rm_list)\n",
    "    keep_list = [item for item in dat.columns if item not in rm_list]\n",
    "    return(dat[keep_list])\n",
    "    \n",
    "\n",
    "filename = prefix_market + \"_all_download_data_01-01-2005_12-31-2019_Adj Close.csv\"\n",
    "origdat = pd.read_csv(os.path.join(base_os,filename),sep=\",\",index_col='date',parse_dates = True)\n",
    "origdat = rm_extreme_price(origdat)\n",
    "print(\"The size of \\\"{}\\\" is: {}\".format(filename, origdat.shape))\n",
    "\n",
    "filename = prefix_market + \"_all_download_data_01-01-2005_12-31-2019_Adj Open.csv\"\n",
    "origdat_o = pd.read_csv(os.path.join(base_os,filename),sep=\",\",index_col='date',parse_dates = True)\n",
    "origdat_o = rm_extreme_price(origdat_o)\n",
    "print(\"The size of \\\"{}\\\" is: {}\".format(filename, origdat_o.shape))\n",
    "\n",
    "filename = prefix_market + \"_all_download_data_01-01-2005_12-31-2019_Volume.csv\"\n",
    "origdat_v = pd.read_csv(os.path.join(base_os,filename),sep=\",\",index_col='date',parse_dates = True)\n",
    "print(\"The size of \\\"{}\\\" is: {}\".format(filename, origdat_v.shape))\n",
    "\n",
    "#prefix_market='sp500'\n",
    "filename = prefix_market + \"_index_data_01-01-2000_05-20-2020.csv\"\n",
    "sp500 =  pd.read_csv(os.path.join(base_os,filename),sep=\",\",index_col='date',parse_dates = True)\n",
    "print(\"The size of \\\"{}\\\" is: {}\".format(filename, sp500.shape))\n",
    "\n",
    "\n",
    "sp500 = pd.DataFrame(sp500['Close'])\n",
    "sp500.columns = ['SP500']\n",
    "#sp500.head()\n",
    "origdat = origdat.merge(sp500, how = 'left', left_index=True, right_index=True)\n",
    "#origdat.head()\n",
    "\n",
    "origdat_ret = origdat.pct_change()\n",
    "origdat_ret = origdat_ret[1:]\n",
    "#origdat_ret = origdat\n",
    "print(\"The size of return is:{}\".format(origdat_ret.shape))\n",
    "#origdat_ret.head()\n",
    "\n",
    "\n",
    "##define set periods for train and trade\n",
    "def add_months(sourcedate, months):\n",
    "    month = sourcedate.month - 1 + months\n",
    "    year = sourcedate.year + month // 12\n",
    "    month = month % 12 + 1\n",
    "    day = min(sourcedate.day, monthrange(year,month)[1])\n",
    "    return date(year, month, day)\n",
    "\n",
    "def diff_month(d1, d2):\n",
    "    return -((d1.year - d2.year) * 12 + d1.month - d2.month)\n",
    "\n",
    "def add_years(sourcedate, years):\n",
    "    month = sourcedate.month\n",
    "    year = sourcedate.year+years\n",
    "    day = sourcedate.day\n",
    "    return date(year, month, day)\n",
    "\n",
    "def diff_year(d1,d2):\n",
    "    return -((d1.year-d2.year))\n",
    "\n",
    "\n",
    "#set for training periods\n",
    "first_period1 = [datetime.strptime(item,'%Y-%m-%d') for item in ['2005-01-01', '2008-01-01', '2009-01-01']]\n",
    "nrows1 = diff_year(datetime.strptime('2009-01-01','%Y-%m-%d'),datetime.strptime('2020-01-01','%Y-%m-%d'))+1\n",
    "periods1=[]\n",
    "for i in range(nrows1):\n",
    "    periods1.append(list(map(lambda period: add_years(period,i), first_period1)))\n",
    "    \n",
    "periods1 = pd.DataFrame(periods1)\n",
    "periods1.columns = ['train_start','trad_start','trad_end']\n",
    "periods1 = periods1.apply(pd.to_datetime)\n",
    "#print(\"Head of periods are:\\n\",periods1.head(),\"\\n\")\n",
    "#print(\"Tail of periods are:\\n\",periods1.tail(),\"\\n\")\n",
    "\n",
    "print(\"There are total {} training and trading sets.\".format(periods1.shape[0]))\n",
    "# define data check \n",
    "def data_quality(nrows,periods):\n",
    "    add_symbol = \"SP500\"\n",
    "    set_dict1 = {}\n",
    "    for i in range(nrows):\n",
    "        #choose date in periods1[i][1]\n",
    "        sel_date0 = periods.iloc[i][0]\n",
    "        sel_date = periods.iloc[i][1]\n",
    "        \n",
    "        ###move 1 year head since sub training stop \n",
    "        #sel_date = sel_date.replace(year = sel_date.year - 1)\n",
    "        #print(sel_date)\n",
    "        sel_stocks = origdat_matrix.index[origdat_matrix[sel_date]==1]\n",
    "        #stocks should be the intersections of columnames of origdat_ret and set_stocks\n",
    "        sel_list = list(set(origdat_ret.columns).intersection(set(sel_stocks)))\n",
    "        sel_list.append(add_symbol)\n",
    "        set_dat = origdat_ret[sel_list]\n",
    "        # set in the date\n",
    "        set_dat = set_dat[periods.iloc[i][0]:periods.iloc[i][1]]\n",
    "        ##check the missing values\n",
    "        keep_columns = []\n",
    "        for item in set_dat.columns:\n",
    "            if set_dat[item].isna().sum()/set_dat.shape[0]<=0.1:\n",
    "                keep_columns.append(item)\n",
    "        ##assert add_symbol must in keep_columns\n",
    "        if add_symbol not in keep_columns:\n",
    "            keep_columns.append(add_symbol)\n",
    "        set_dat = set_dat[list(set(set_dat.columns).intersection(set(keep_columns)))]\n",
    "        set_dict1[sel_date0] = sorted(set_dat.columns)\n",
    "    \n",
    "        #print(\"For period {}, there are total {} be selected!\".format(sel_date,set_dat.shape[1]))\n",
    "    return set_dict1\n",
    "#set_dict1\n",
    "set_dict1 = data_quality(nrows1,periods1) \n",
    "\n",
    "def dict_summ(set_dict):\n",
    "    period_st = []\n",
    "    num_stock = []\n",
    "    for keys, value in set_dict.items():\n",
    "        period_st.append(keys)\n",
    "        num_stock.append(len(value))\n",
    "\n",
    "    dict_summ = pd.DataFrame([period_st,num_stock]).T\n",
    "    dict_summ.columns = ['start_time','num_stocks']\n",
    "    return dict_summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since there are 12 periods, so there will be 12 sample dataset as train and trade, we pick up one as an example for pre-processing and training data development** <br>\n",
    "The following steps are similar to several steps of EDA. If you have any questions/concerns, please review here: <br>\n",
    "https://github.com/jiaqixu/Springboard/blob/master/Capstone/Capstone1/exploratory_data_analysis_capstone1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                A  AABA   AAPL    ABC    ABT\n",
      "date                                        \n",
      "2007-01-03  22.40  6.98  10.39  18.34  13.73\n",
      "2007-01-04  22.47  7.32  10.62  18.41  13.99\n",
      "2007-01-05  22.26  7.56  10.55  18.19  13.99\n",
      "2007-01-08  22.18  7.61  10.60  18.48  14.04\n",
      "2007-01-09  22.21  7.52  11.48  18.42  14.17\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240]\n",
      "create features for training data.\n",
      "240 280 320 360 400 440 480 520 560 600 640 680 720 \n",
      "\n",
      "The size of train data set is: (197760, 31)\n",
      "The head of train data set is:\n",
      "\n",
      "            f1        f2        f3        f4        f5        f6        f7  \\\n",
      "A    -0.017406 -0.021057 -0.004200 -0.041633 -0.044722 -0.043180 -0.040469   \n",
      "AABA -0.013534 -0.019432 -0.016492 -0.045124 -0.061516 -0.073446 -0.073446   \n",
      "AAPL -0.007566 -0.002535  0.009837 -0.019518 -0.019925  0.002548  0.026522   \n",
      "ABC  -0.019912 -0.032223 -0.028509 -0.064908 -0.066877 -0.049356 -0.036957   \n",
      "ABT  -0.015107 -0.010508 -0.009930 -0.023618 -0.012238 -0.011085 -0.011085   \n",
      "\n",
      "            f8        f9       f10  ...       f60       f80      f100  \\\n",
      "A    -0.031058 -0.038134 -0.040081  ... -0.019032  0.056125 -0.072379   \n",
      "AABA -0.088889 -0.095172 -0.102599  ... -0.047896  0.036335 -0.025260   \n",
      "AAPL  0.058744  0.064472  0.044690  ...  0.356897  0.437005  0.387192   \n",
      "ABC  -0.020994 -0.027442 -0.056443  ... -0.057447 -0.085183 -0.105502   \n",
      "ABT   0.000000  0.008929  0.000000  ...  0.084453  0.117337  0.113666   \n",
      "\n",
      "          f120      f140      f160      f180      f200      f220      f240  \n",
      "A    -0.054248 -0.052358  0.056595  0.077727  0.181954  0.127973  0.058482  \n",
      "AABA -0.131126 -0.152455 -0.141361 -0.230950 -0.208685 -0.151358 -0.060172  \n",
      "AAPL  0.590970  0.665021  0.907108  1.049479  1.229462  1.246432  1.272377  \n",
      "ABC  -0.102785 -0.152153 -0.110442 -0.156993 -0.139806 -0.149304 -0.033806  \n",
      "ABT   0.092843  0.041155  0.041795  0.070076  0.126246  0.129247  0.234523  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "create classification for target data.\n",
      "240 280 320 360 400 440 480 520 560 600 640 680 720 \n",
      "\n",
      "The size of target data set is: (197760, 1)\n",
      "The head of target data set is:\n",
      "\n",
      "        0\n",
      "A     0.0\n",
      "AABA  0.0\n",
      "AAPL  0.0\n",
      "ABC   1.0\n",
      "ABT   1.0\n"
     ]
    }
   ],
   "source": [
    "train_ind = 2\n",
    "cur_key = set_dict1[periods1.iloc[train_ind,0]]\n",
    "start_date = periods1.iloc[train_ind,0]\n",
    "end_date = periods1.iloc[train_ind,1]\n",
    "\n",
    "cur_dat = origdat.loc[start_date:end_date,cur_key]\n",
    "#cur_dat.fillna(method = 'ffill',inplace=True)\n",
    "#fill na by 'ffill' method\n",
    "cur_dat.fillna(method = 'ffill',inplace=True)\n",
    "print(cur_dat.iloc[:5,:5])\n",
    "\n",
    "\n",
    "m = list(range(1,21))\n",
    "for k in range(2,13):\n",
    "    m.append(k*20)\n",
    "print(m)\n",
    "\n",
    "def lag_return(dat,start_row,lag):\n",
    "    return dat[start_row]/dat[start_row-lag]-1\n",
    "\n",
    "def future_return(dat,start_row,fut):\n",
    "    return dat[start_row+fut]/dat[start_row]-1\n",
    "\n",
    "\n",
    "def create_feature_once(dat, start_ind, sel_feat):\n",
    "    df = pd.DataFrame()\n",
    "    #for i in range(len(sel_feat)):\n",
    "    for ind in sel_feat:\n",
    "        #print(ind)\n",
    "        feat_ind = dat.apply(lag_return,args=(start_ind,ind),axis=0)\n",
    "        feat_ind = feat_ind.to_frame()\n",
    "        feat_ind = feat_ind.rename(columns={0:'f'+str(ind)})\n",
    "        df = pd.concat([df,feat_ind],axis=1)\n",
    "    return df\n",
    "\n",
    "def create_target_once(dat,start_ind,sel_fut):\n",
    "    target = dat.apply(future_return,args=(start_ind,sel_fut),axis=0)\n",
    "    set_median = target.median()\n",
    "    target = target.apply(lambda x:1 if x>set_median else 0)\n",
    "    #target = target.to_frame()\n",
    "    #target = target.rename(columns={0:'tar'+str(start_ind)})\n",
    "    return(target)\n",
    "\n",
    "\n",
    "def create_target_once2(dat,start_ind,sel_fut):\n",
    "    target = dat.apply(future_return,args=(start_ind,sel_fut),axis=0)\n",
    "    set_median = target.median()\n",
    "    target = target.apply(lambda x:1 if x>0 else 0)\n",
    "    #target = target.to_frame()\n",
    "    #target = target.rename(columns={0:'tar'+str(start_ind)})\n",
    "    return(target)\n",
    "\n",
    "def create_feature_all(dat, sel_feat):\n",
    "    print(\"create features for training data.\")\n",
    "    df = pd.DataFrame()\n",
    "    total_dat = dat.shape[0]\n",
    "    end_ind = total_dat - 1\n",
    "    ind = max(sel_feat)\n",
    "    #n =0 \n",
    "    while ind<=end_ind-1:\n",
    "        if ind%40==0:\n",
    "            print(ind, end = \" \")\n",
    "        df_feat = create_feature_once(dat,start_ind= ind, sel_feat=sel_feat)\n",
    "        #if n==0:\n",
    "            #df_feat = create_feature_once(cur_dat,start_ind= ind, sel_feat=m)\n",
    "        #    df = df_feat\n",
    "        #else:\n",
    "        df = pd.concat([df,df_feat],axis=0)\n",
    "        #    df = np.hstack([df,df_feat])\n",
    "        ind = ind + 1\n",
    "    print(\"\\n\")\n",
    "    return(df)\n",
    "\n",
    "def create_target_all(dat,sel_fut,sel_feat):\n",
    "    print(\"create classification for target data.\")\n",
    "    df = pd.DataFrame()\n",
    "    total_dat = dat.shape[0]\n",
    "    end_ind = total_dat - 1\n",
    "    ind = max(sel_feat)\n",
    "    while ind<=end_ind-sel_fut:\n",
    "        if ind%40==0:\n",
    "            print(ind, end = \" \")\n",
    "        df_target = create_target_once(dat,start_ind= ind, sel_fut=sel_fut)\n",
    "        #df_target = create_target_once2(cur_dat,start_ind= ind, sel_fut=sel_fut)\n",
    "        df = pd.concat([df,df_target],axis=0)\n",
    "        ind = ind + 1\n",
    "    print(\"\\n\")\n",
    "    return(df)\n",
    "\n",
    "\n",
    "# create features\n",
    "df_feat = create_feature_all(dat = cur_dat, sel_feat=m)\n",
    "#print(df_feat.head(5))\n",
    "print(\"The size of train data set is:\", df_feat.shape)\n",
    "print(\"The head of train data set is:\\n\")\n",
    "print(df_feat.head(5))\n",
    "\n",
    "df_target = create_target_all(cur_dat,sel_fut=1,sel_feat=m)\n",
    "print(\"The size of target data set is:\", df_target.shape)\n",
    "print(\"The head of target data set is:\\n\")\n",
    "print(df_target.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data size for X_train is: (158208, 31)\n",
      "The data size for y_train is: (158208,)\n",
      "The data size for X_test is: (39552, 31)\n",
      "The data size for y_test is: (39552,)\n"
     ]
    }
   ],
   "source": [
    "X = df_feat\n",
    "#print(X.mean())\n",
    "y = df_target\n",
    "#scaler = preprocessing.StandardScaler().fit(X)\n",
    "# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X \n",
    "#X=scaler.transform(X)\n",
    "y = np.ravel(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,shuffle=True, random_state=42)\n",
    "print(\"The data size for X_train is:\", X_train.shape)\n",
    "print(\"The data size for y_train is:\", y_train.shape)\n",
    "print(\"The data size for X_test is:\", X_test.shape)\n",
    "print(\"The data size for y_test is:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler for X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Data\\Machine_Learning_Library\\learn_others\\learn_from_others\\lib\\site-packages\\pandas\\core\\generic.py:6287: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "X_train.fillna(X_train.mean(),inplace=True)\n",
    "X_test.fillna(X_test.mean(),inplace=True)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns, index =X.index[:round(X.shape[0]*0.8)]) \n",
    "X_test = pd.DataFrame(X_test, columns=X.columns, index =X.index[round(X.shape[0]*0.8):]) \n",
    "y_train = pd.DataFrame(y_train, columns=[\"Target\"], index =X.index[:round(X.shape[0]*0.8)])\n",
    "y_test = pd.DataFrame(y_test, columns=[\"Target\"], index =X.index[round(X.shape[0]*0.8):])\n",
    "X_train.to_csv(\"Train_set\"+str(train_ind)+\"_Feature.csv\")\n",
    "y_train.to_csv(\"Train_set\"+str(train_ind)+\"_Target.csv\")\n",
    "X_test.to_csv(\"Test_set\"+str(train_ind)+\"_Feature.csv\")\n",
    "y_test.to_csv(\"Test_set\"+str(train_ind)+\"_Target.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=2000, max_depth=30, max_features = 'log2', max_samples = 700, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Training accuracy:\", round(clf.score(X_train,y_train),4))\n",
    "print(\"Prediction accuracy:\", round(clf.score(X_test,y_test),4))\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "print(\"Confusion matrix:\\n\", pd.DataFrame(cnf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_from_others",
   "language": "python",
   "name": "learn_from_others"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
